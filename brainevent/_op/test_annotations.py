# Copyright 2026 BrainX Ecosystem Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Test // @BE annotation-based function discovery."""

import jax
import jax.numpy as jnp
import numpy as np
import pytest
from brainevent._test_util import requires_gpu

pytestmark = requires_gpu

from brainevent._op._codegen import normalize_tokens

from brainevent._error import KernelError
from brainevent._op._codegen import parse_annotations
from brainevent._op._pipeline import load_cuda_inline


# --- Unit tests for normalize_tokens (no GPU needed) ---

def test_normalize_tokens_legacy_aliases():
    """normalize_tokens converts kernel aliases to canonical BE tokens."""

    assert normalize_tokens(["args", "rets", "ctx.stream"]) == ["arg", "ret", "stream"]


def test_normalize_tokens_attrs_prefix():
    """attrs.name → attr.name (bare form)."""

    result = normalize_tokens(["attrs.scale", "attrs.offset"])
    assert result == ["attr.scale", "attr.offset"]


def test_normalize_tokens_passthrough():
    """Already-canonical tokens are not modified."""

    tokens = ["arg", "ret", "stream", "attr.x:float32", "attr.y"]
    assert normalize_tokens(tokens) == tokens


def test_normalize_tokens_mixed():
    """Mixed JKB and kernel tokens all normalise correctly."""

    result = normalize_tokens(["args", "rets", "attrs.scale", "ctx.stream"])
    assert result == ["arg", "ret", "attr.scale", "stream"]


# --- Unit tests for annotation parsing (no GPU needed, but in same file) ---

def test_parse_annotations_basic():
    """parse_be_annotations finds annotated functions."""

    source = """
    // @BE my_add
    void my_add(const BE::Tensor a, const BE::Tensor b,
                BE::Tensor out, int64_t stream) {
    }
    """
    result = parse_annotations(source)
    assert "my_add" in result
    assert result["my_add"] == ["arg", "arg", "ret", "stream"]


def test_parse_annotations_no_stream():
    """Annotation correctly infers arg_spec without stream."""

    source = """
    // @BE add_cpu
    void add_cpu(const BE::Tensor x, BE::Tensor y) {}
    """
    result = parse_annotations(source)
    assert result["add_cpu"] == ["arg", "ret"]


def test_parse_annotations_multiple():
    """Multiple annotations in the same source."""

    source = """
    // @BE func_a
    void func_a(const BE::Tensor x, BE::Tensor out) {}

    // @BE func_b
    void func_b(const BE::Tensor a, const BE::Tensor b,
                BE::Tensor out, int64_t stream) {}
    """
    result = parse_annotations(source)
    assert set(result.keys()) == {"func_a", "func_b"}
    assert result["func_a"] == ["arg", "ret"]
    assert result["func_b"] == ["arg", "arg", "ret", "stream"]


def test_parse_annotations_no_annotations():
    """Raises when no annotations found."""

    with pytest.raises(KernelError, match="No '// @BE"):
        parse_annotations("void foo() {}")


def test_parse_annotations_duplicate():
    """Raises on duplicate @BE annotation."""

    source = """
    // @BE same_name
    void same_name(const BE::Tensor x, BE::Tensor out) {}
    // @BE same_name
    void same_name(const BE::Tensor x, BE::Tensor out) {}
    """
    with pytest.raises(KernelError, match="Duplicate"):
        parse_annotations(source)


def test_parse_annotations_macro_generated():
    """parse_be_annotations discovers functions generated by token-pasting macros.

    When a function is produced by a macro (e.g. MY_KERNEL(_f32_bool, ...)),
    its full signature does not appear verbatim.  The fallback traces
    @BE annotation → macro call → #define body to extract the parameter list.
    """

    source = r"""
    #define MY_KERNEL(SUFFIX, WEIGHT_T, SPIKE_T)    \
    void my_kernel##SUFFIX(                           \
        const BE::Tensor weights,                     \
        const BE::Tensor spikes,                      \
        BE::Tensor output,                            \
        int64_t stream                                \
    ) { /* body */ }

    // @BE my_kernel_f32_bool
    MY_KERNEL(_f32_bool, float, uint8_t)
    """
    result = parse_annotations(source)
    assert "my_kernel_f32_bool" in result
    assert result["my_kernel_f32_bool"] == ["arg", "arg", "ret", "stream"]


def test_parse_annotations_macro_multiple_instantiations():
    """Multiple macro instantiations each with their own @BE annotation."""

    source = r"""
    #define KERNEL(SUFFIX, WT)         \
    void kern##SUFFIX(                 \
        const BE::Tensor w,            \
        BE::Tensor out,                \
        int64_t stream                 \
    ) { }

    // @BE kern_f32
    KERNEL(_f32, float)

    // @BE kern_f64
    KERNEL(_f64, double)
    """
    result = parse_annotations(source)
    assert set(result.keys()) == {"kern_f32", "kern_f64"}
    assert result["kern_f32"] == ["arg", "ret", "stream"]
    assert result["kern_f64"] == ["arg", "ret", "stream"]


def test_parse_annotations_inline_spec():
    """// @BE func_name arg arg ret stream uses inline tokens as arg_spec."""

    # The inline spec bypasses any C++ signature lookup entirely.
    source = """
    // @BE my_kernel arg arg ret stream
    void my_kernel(const BE::Tensor w, const BE::Tensor s,
                   BE::Tensor out, int64_t stream) {}
    """
    result = parse_annotations(source)
    assert result["my_kernel"] == ["arg", "arg", "ret", "stream"]


def test_parse_annotations_inline_spec_no_stream():
    """Inline spec without stream token."""

    source = """
    // @BE simple_kernel arg ret
    void simple_kernel(const BE::Tensor x, BE::Tensor out) {}
    """
    result = parse_annotations(source)
    assert result["simple_kernel"] == ["arg", "ret"]


def test_parse_annotations_inline_spec_legacy_aliases():
    """Inline spec accepts kernel token aliases and normalises them."""

    source = """
    // @BE my_func args rets ctx.stream
    void my_func(const BE::Tensor x, BE::Tensor out, int64_t stream) {}
    """
    result = parse_annotations(source)
    # aliases are normalised to canonical BE form
    assert result["my_func"] == ["arg", "ret", "stream"]


def test_parse_annotations_inline_spec_macro_generated():
    """Inline spec on a macro-generated function (no C++ signature in source)."""

    source = r"""
    #define GEN(SUFFIX, WT, ST)        \
    void gen##SUFFIX(                  \
        const BE::Tensor w,            \
        const BE::Tensor s,            \
        BE::Tensor out,                \
        int64_t stream                 \
    ) { }

    // @BE gen_f32_bool arg arg ret stream
    GEN(_f32_bool, float, uint8_t)
    """
    result = parse_annotations(source)
    # Inline spec takes precedence — no C++ inference needed
    assert result["gen_f32_bool"] == ["arg", "arg", "ret", "stream"]


# --- Integration test: compile and run a CUDA kernel via annotations ---

CUDA_SRC = r"""
#include <cuda_runtime.h>
#include "brainevent/common.h"

__global__ void add_k(const float* a, const float* b, float* o, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) o[i] = a[i] + b[i];
}

// @BE vector_add
void vector_add(const BE::Tensor a, const BE::Tensor b,
                BE::Tensor out, int64_t stream) {
    int n = a.numel();
    add_k<<<(n+255)/256, 256, 0, (cudaStream_t)stream>>>(
        static_cast<const float*>(a.data_ptr()),
        static_cast<const float*>(b.data_ptr()),
        static_cast<float*>(out.data_ptr()), n);
}
"""


@pytest.fixture(scope="module")
def annotation_module():
    return load_cuda_inline(
        name="test_annotation_vadd",
        cuda_sources=CUDA_SRC,
        # functions=None  →  discovered from // @BE annotations
        force_rebuild=True,
    )


def test_annotation_cuda_kernel(annotation_module):
    """CUDA kernel compiled via annotation produces correct result."""

    a = jnp.ones(512, dtype=jnp.float32)
    b = jnp.full(512, 2.0, dtype=jnp.float32)

    result = jax.ffi.ffi_call(
        "test_annotation_vadd.vector_add",
        jax.ShapeDtypeStruct((512,), jnp.float32),
    )(a, b)

    expected = np.full(512, 3.0, dtype=np.float32)
    np.testing.assert_allclose(np.asarray(result), expected, rtol=1e-5)
